{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Wave-U-net.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNZnuv8jiKS20LkFXDggXAL",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/satvik-venkatesh/Wave-U-net-TF2/blob/main/Wave-U-net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6b9K3J4ZEdqV"
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Layer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAyowzcelZdt"
   },
   "source": [
    "# Custom layers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LFkCcACAQJA3"
   },
   "source": [
    "class AudioClipLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        '''Initializes the instance attributes'''\n",
    "        super(AudioClipLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Create the state of the layer (weights)'''\n",
    "        # initialize the weights\n",
    "        pass\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        '''Defines the computation from inputs to outputs'''\n",
    "        if training:\n",
    "            return inputs\n",
    "        else:\n",
    "            return tf.maximum(tf.minimum(inputs, 1.0), -1.0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Woi9w83c93fG"
   },
   "source": [
    "# Learned Interpolation layer\n",
    "\n",
    "class InterpolationLayer(Layer):\n",
    "\n",
    "    def __init__(self, padding = \"valid\", **kwargs):\n",
    "        '''Initializes the instance attributes'''\n",
    "        super(InterpolationLayer, self).__init__(**kwargs)\n",
    "        self.padding = padding\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Create the state of the layer (weights)'''\n",
    "        self.features = input_shape.as_list()[3]\n",
    "\n",
    "        # initialize the weights\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(name=\"kernel\",\n",
    "            initial_value=w_init(shape=(self.features, ),\n",
    "                                 dtype='float32'),\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''Defines the computation from inputs to outputs'''\n",
    "\n",
    "        w_scaled = tf.math.sigmoid(self.w)\n",
    "\n",
    "        counter_w = 1 - w_scaled\n",
    "\n",
    "        conv_weights = tf.expand_dims(tf.concat([tf.expand_dims(tf.linalg.diag(w_scaled), axis=0), tf.expand_dims(tf.linalg.diag(counter_w), axis=0)], axis=0), axis=0)\n",
    "\n",
    "        intermediate_vals = tf.nn.conv2d(inputs, conv_weights, strides=[1,1,1,1], padding=self.padding.upper())\n",
    "\n",
    "        intermediate_vals = tf.transpose(intermediate_vals, [2, 0, 1, 3])\n",
    "        out = tf.transpose(inputs, [2, 0, 1, 3])\n",
    "        \n",
    "        num_entries = out.shape.as_list()[0]\n",
    "        out = tf.concat([out, intermediate_vals], axis=0)\n",
    "\n",
    "        indices = list()\n",
    "\n",
    "        # num_outputs = 2*num_entries - 1\n",
    "        num_outputs = (2*num_entries - 1) if self.padding == \"valid\" else 2*num_entries\n",
    "\n",
    "        for idx in range(num_outputs):\n",
    "            if idx % 2 == 0:\n",
    "                indices.append(idx // 2)\n",
    "            else:\n",
    "                indices.append(num_entries + idx//2)\n",
    "        out = tf.gather(out, indices)\n",
    "        current_layer = tf.transpose(out, [1, 2, 0, 3])\n",
    "\n",
    "        return current_layer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9onhDb0lTCQp"
   },
   "source": [
    "class CropLayer(Layer):\n",
    "    def __init__(self, x2, match_feature_dim=True, **kwargs):\n",
    "        '''Initializes the instance attributes'''\n",
    "        super(CropLayer, self).__init__(**kwargs)\n",
    "        self.match_feature_dim = match_feature_dim\n",
    "        self.x2 = x2\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Create the state of the layer (weights)'''\n",
    "        # initialize the weights\n",
    "        pass\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        '''Defines the computation from inputs to outputs'''\n",
    "        if self.x2 is None:\n",
    "            return inputs\n",
    "\n",
    "        inputs = self.crop(inputs, self.x2.shape.as_list(), self.match_feature_dim)\n",
    "        return inputs\n",
    "\n",
    "    def crop(self, tensor, target_shape, match_feature_dim=True):\n",
    "        '''\n",
    "        Crops a 3D tensor [batch_size, width, channels] along the width axes to a target shape.\n",
    "        Performs a centre crop. If the dimension difference is uneven, crop last dimensions first.\n",
    "        :param tensor: 4D tensor [batch_size, width, height, channels] that should be cropped. \n",
    "        :param target_shape: Target shape (4D tensor) that the tensor should be cropped to\n",
    "        :return: Cropped tensor\n",
    "        '''\n",
    "        shape = np.array(tensor.shape.as_list())\n",
    "\n",
    "        ddif = shape[1] - target_shape[1]\n",
    "\n",
    "        if (ddif % 2 != 0):\n",
    "            print(\"WARNING: Cropping with uneven number of extra entries on one side\")\n",
    "        # assert diff[1] >= 0 # Only positive difference allowed\n",
    "        if ddif == 0:\n",
    "            return tensor\n",
    "        crop_start = ddif // 2\n",
    "        crop_end = ddif - crop_start\n",
    "\n",
    "        return tensor[:,crop_start:-crop_end,:]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1qlZHtgp-6aF"
   },
   "source": [
    "class IndependentOutputLayer(Layer):\n",
    "\n",
    "    def __init__(self, source_names, num_channels, filter_width, padding=\"valid\", **kwargs):\n",
    "        '''Initializes the instance attributes'''\n",
    "        super(IndependentOutputLayer, self).__init__(**kwargs)\n",
    "        self.source_names = source_names\n",
    "        self.num_channels = num_channels\n",
    "        self.filter_width = filter_width\n",
    "        self.padding = padding\n",
    "\n",
    "        self.conv1a = tf.keras.layers.Conv1D(self.num_channels, self.filter_width, padding= self.padding)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Create the state of the layer (weights)'''\n",
    "        pass\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        '''Defines the computation from inputs to outputs'''\n",
    "        outputs = {}\n",
    "        for name in self.source_names:\n",
    "            out = self.conv1a(inputs)\n",
    "            outputs[name] = out\n",
    "        \n",
    "        return outputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oxLPPJ1Lasvn"
   },
   "source": [
    "class DiffOutputLayer(Layer):\n",
    "\n",
    "    def __init__(self, source_names, num_channels, filter_width, padding=\"valid\", **kwargs):\n",
    "        '''Initializes the instance attributes'''\n",
    "        super(DiffOutputLayer, self).__init__(**kwargs)\n",
    "        self.source_names = source_names\n",
    "        self.num_channels = num_channels\n",
    "        self.filter_width = filter_width\n",
    "        self.padding = padding\n",
    "\n",
    "        self.conv1a = tf.keras.layers.Conv1D(self.num_channels, self.filter_width, padding= self.padding)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Create the state of the layer (weights)'''\n",
    "        pass\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        '''Defines the computation from inputs to outputs'''\n",
    "        outputs = {}\n",
    "        sum_source = 0\n",
    "        for name in self.source_names[:-1]:\n",
    "            out = self.conv1a(inputs[0])\n",
    "            out = AudioClipLayer()(out)\n",
    "            outputs[name] = out\n",
    "            sum_source = sum_source + out\n",
    "        \n",
    "        last_source = CropLayer(sum_source)(inputs[1]) - sum_source\n",
    "        last_source = AudioClipLayer()(last_source)\n",
    "\n",
    "        outputs[self.source_names[-1]] = last_source\n",
    "\n",
    "        return outputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV6T2ttZlg7Q"
   },
   "source": [
    "# Define the Network"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_4MYB1pafSfI"
   },
   "source": [
    "def wave_u_net(num_initial_filters = 24, num_layers = 12, kernel_size = 15, merge_filter_size = 5, \n",
    "               source_names = [\"bass\", \"drums\", \"other\", \"vocals\"], num_channels = 1, output_filter_size = 1,\n",
    "               padding = \"same\", input_size = 16384 * 4, context = False, upsampling_type = \"learned\",\n",
    "               output_activation = \"linear\", output_type = \"difference\"):\n",
    "  \n",
    "  # `enc_outputs` stores the downsampled outputs to re-use during upsampling.\n",
    "  enc_outputs = []\n",
    "\n",
    "  # `raw_input` is the input to the network\n",
    "  raw_input = tf.keras.layers.Input(shape=(input_size, num_channels),name=\"raw_input\")\n",
    "  X = raw_input\n",
    "  inp = raw_input\n",
    "\n",
    "  # Down sampling\n",
    "  for i in range(num_layers):\n",
    "    X = tf.keras.layers.Conv1D(filters=num_initial_filters + (num_initial_filters * i),\n",
    "                          kernel_size=kernel_size,strides=1,\n",
    "                          padding=padding, name=\"Down_Conv_\"+str(i))(X)\n",
    "    X = tf.keras.layers.LeakyReLU(name=\"Down_Conv_Activ_\"+str(i))(X)\n",
    "\n",
    "    enc_outputs.append(X)\n",
    "\n",
    "    X = tf.keras.layers.Lambda(lambda x: x[:,::2,:], name=\"Decimate_\"+str(i))(X)\n",
    "\n",
    "\n",
    "  X = tf.keras.layers.Conv1D(filters=num_initial_filters + (num_initial_filters * num_layers),\n",
    "                          kernel_size=kernel_size,strides=1,\n",
    "                          padding=padding, name=\"Down_Conv_\"+str(num_layers))(X)\n",
    "  X = tf.keras.layers.LeakyReLU(name=\"Down_Conv_Activ_\"+str(num_layers))(X)\n",
    "\n",
    "\n",
    "\n",
    "  # Up sampling\n",
    "  for i in range(num_layers):\n",
    "    X = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=1), name=\"exp_dims_\"+str(i))(X)\n",
    "    \n",
    "    if upsampling_type == \"learned\":\n",
    "      X = InterpolationLayer(name=\"IntPol_\"+str(i), padding=padding)(X)\n",
    "\n",
    "    else:\n",
    "      if context:\n",
    "        X = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [1, x.shape.as_list()[2] * 2 - 1]), name=\"bilinear_interpol_\"+str(i))(X)\n",
    "        # current_layer = tf.image.resize_bilinear(current_layer, [1, current_layer.get_shape().as_list()[2] * 2 - 1], align_corners=True)\n",
    "      else:\n",
    "        X = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [1, x.shape.as_list()[2] * 2]), name=\"bilinear_interpol_\"+str(i))(X)\n",
    "        # current_layer = tf.image.resize_bilinear(current_layer, [1, current_layer.get_shape().as_list()[2]*2]) # out = in + in - 1\n",
    "\n",
    "\n",
    "    X = tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=1), name=\"sq_dims_\"+str(i))(X)\n",
    "    \n",
    "    c_layer = CropLayer(X, False, name=\"crop_layer_\"+str(i))(enc_outputs[-i-1])\n",
    "    X = tf.keras.layers.Concatenate(axis=2, name=\"concatenate_\"+str(i))([X, c_layer]) \n",
    "\n",
    "\n",
    "    X = tf.keras.layers.Conv1D(filters=num_initial_filters + (num_initial_filters * (num_layers - i - 1)),\n",
    "                            kernel_size=merge_filter_size,strides=1,\n",
    "                            padding=padding, name=\"Up_Conv_\"+str(i))(X)\n",
    "    X = tf.keras.layers.LeakyReLU(name=\"Up_Conv_Activ_\"+str(i))(X)\n",
    "\n",
    "\n",
    "  c_layer = CropLayer(X, False, name=\"crop_layer_\"+str(num_layers))(inp)\n",
    "  X = tf.keras.layers.Concatenate(axis=2, name=\"concatenate_\"+str(num_layers))([X, c_layer]) \n",
    "  X = AudioClipLayer(name=\"audio_clip_\"+str(0))(X)\n",
    "\n",
    "  if output_type == \"direct\":\n",
    "    X = IndependentOutputLayer(source_names, num_channels, output_filter_size, padding=padding, name=\"independent_out\")(X)\n",
    "\n",
    "  else:\n",
    "    # Difference Output\n",
    "    cropped_input = CropLayer(X, False, name=\"crop_layer_\"+str(num_layers+1))(inp)\n",
    "    X = DiffOutputLayer(source_names, num_channels, output_filter_size, padding=padding, name=\"diff_out\")([X, cropped_input])\n",
    "\n",
    "  o = X\n",
    "  model = tf.keras.Model(inputs=raw_input, outputs=o)\n",
    "  return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cRqjX5C5APAX"
   },
   "source": [
    "# Parameters for the Wave-U-net\n",
    "\n",
    "params = {\n",
    "  \"num_initial_filters\": 24,\n",
    "  \"num_layers\": 12,\n",
    "  \"kernel_size\": 15,\n",
    "  \"merge_filter_size\": 5,\n",
    "  \"source_names\": [\"bass\", \"drums\", \"other\", \"vocals\"],\n",
    "  \"num_channels\": 2,\n",
    "  \"output_filter_size\": 1,\n",
    "  \"padding\": \"valid\",\n",
    "  \"input_size\": 147443,\n",
    "  \"context\": True,\n",
    "  \"upsampling_type\": \"learned\",         # \"learned\" or \"linear\"\n",
    "  \"output_activation\": \"linear\",        # \"linear\" or \"tanh\"\n",
    "  \"output_type\": \"difference\",          # \"direct\" or \"difference\" \n",
    "}"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tPc2_kxuf_xE"
   },
   "source": [
    "m = wave_u_net(**params)"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEZbrek4mqwg",
    "outputId": "319cb542-34b5-4ee3-e2f2-fdd05823a557"
   },
   "source": [
    "m.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUQ3cplMm8Hf",
    "outputId": "cbe17e09-1e49-468a-9d46-87522ad855ee"
   },
   "source": [
    "m.output"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WfjeTMngEI-u"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2xXpQWolnyY"
   },
   "source": [
    "# Other utility functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HxhDdi8SfXRS"
   },
   "source": [
    "def get_padding(shape, num_layers=12, filter_size=15, input_filter_size=15, output_filter_size=1, merge_filter_size=5, num_channels=1, context = True):\n",
    "    '''\n",
    "    Note that this function is not used within the Wave-U-net. \n",
    "    But it is useful to calculate the required amounts of padding along \n",
    "    each axis of the input and output, so that the Unet works and has the \n",
    "    given shape as output shape.\n",
    "\n",
    "    :param shape: Desired output shape \n",
    "    :return: Input_shape, output_shape, where each is a list [batch_size, time_steps, channels]\n",
    "    '''\n",
    "\n",
    "    if context:\n",
    "        # Check if desired shape is possible as output shape - go from output shape towards lowest-res feature map\n",
    "        rem = float(shape[1]) # Cut off batch size number and channel\n",
    "\n",
    "        # Output filter size\n",
    "        rem = rem - output_filter_size + 1\n",
    "\n",
    "        # Upsampling blocks\n",
    "        for i in range(num_layers):\n",
    "            rem = rem + merge_filter_size - 1\n",
    "            rem = (rem + 1.) / 2.# out = in + in - 1 <=> in = (out+1)/\n",
    "\n",
    "        # Round resulting feature map dimensions up to nearest integer\n",
    "        x = np.asarray(np.ceil(rem),dtype=np.int64)\n",
    "        assert(x >= 2)\n",
    "\n",
    "        # Compute input and output shapes based on lowest-res feature map\n",
    "        output_shape = x\n",
    "        input_shape = x\n",
    "\n",
    "        # Extra conv\n",
    "        input_shape = input_shape + filter_size - 1\n",
    "\n",
    "        # Go from centre feature map through up- and downsampling blocks\n",
    "        for i in range(num_layers):\n",
    "            output_shape = 2*output_shape - 1 #Upsampling\n",
    "            output_shape = output_shape - merge_filter_size + 1 # Conv\n",
    "\n",
    "            input_shape = 2*input_shape - 1 # Decimation\n",
    "            if i < num_layers - 1:\n",
    "                input_shape = input_shape + filter_size - 1 # Conv\n",
    "            else:\n",
    "                input_shape = input_shape + input_filter_size - 1\n",
    "\n",
    "        # Output filters\n",
    "        output_shape = output_shape - output_filter_size + 1\n",
    "\n",
    "        input_shape = np.concatenate([[shape[0]], [input_shape], [num_channels]])\n",
    "        output_shape = np.concatenate([[shape[0]], [output_shape], [num_channels]])\n",
    "\n",
    "        return input_shape, output_shape\n",
    "    else:\n",
    "        return [shape[0], shape[1], num_channels], [shape[0], shape[1], num_channels]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ptkj9jCVid0T",
    "outputId": "de3cce66-82be-4718-e26e-fd74c1f13b5a"
   },
   "source": [
    "get_padding((16, 16389, 1))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
